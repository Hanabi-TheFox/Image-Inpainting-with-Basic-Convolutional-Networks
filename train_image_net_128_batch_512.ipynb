{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import pytorch_lightning as pl\n",
    "from image_inpainting.datamodule.image_net_data_module import ImageNetDataModule\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from image_inpainting.model.context_encoder import ContextEncoder\n",
    "from image_inpainting.utils import print_results_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "dm = ImageNetDataModule(\n",
    "    data_dir=os.path.join(data_dir, \"imagenet-128\"), \n",
    "    batch_size_train=512,\n",
    "    batch_size_val=512,\n",
    "    batch_size_test=512,\n",
    "    num_workers=10, \n",
    "    pin_memory=True, \n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a ContextEncoder model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContextEncoder(input_size=(3, 128, 128), hidden_size=4000, save_image_per_epoch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or load it from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextEncoder(\n",
       "  (psnr_metric): PeakSignalNoiseRatio()\n",
       "  (joint_loss): JointLoss(\n",
       "    (rec_loss): ReconstructionLoss()\n",
       "    (adv_loss): AdversarialLoss(\n",
       "      (loss_function): BCELoss()\n",
       "    )\n",
       "  )\n",
       "  (rec_loss): ReconstructionLoss()\n",
       "  (adv_loss): AdversarialLoss(\n",
       "    (loss_function): BCELoss()\n",
       "  )\n",
       "  (generator): AdversarialGenerator(\n",
       "    (encoder): Encoder(\n",
       "      (encoder): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (8): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (11): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (13): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (14): Conv2d(512, 4000, kernel_size=(4, 4), stride=(1, 1))\n",
       "        (15): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (decoder): Sequential(\n",
       "        (0): ConvTranspose2d(4000, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): ReLU(inplace=True)\n",
       "        (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (13): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (discriminator): AdversarialDiscriminator(\n",
       "    (discriminator): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "      (12): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ContextEncoder.load_from_checkpoint(\"checkpoints/imagenet_128/2024-12-17_17-07-56-epoch=42-val_loss=0.42.ckpt\") # change the path to your checkpoint\n",
    "model.enable_save_image_per_epoch()\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints/imagenet_128',\n",
    "    filename=now+'-{epoch:02d}-{val_loss:.2f}',\n",
    "    monitor='val_loss',\n",
    "    save_top_k=-1,  # Save all checkpoints\n",
    "    every_n_epochs=1  # Save checkpoint every n epochs\n",
    ")\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(\"Context_Encoder_Inpainting\")\n",
    "trainer = pl.Trainer(max_epochs=300, devices=-1, accelerator=\"cuda\", logger=tb_logger, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: The ImageNet dataset can't be downloaded automatically. Please refer to the README if you haven't already downloaded it. Otherwise you can skip this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\CYTechNVME\\AIIP_Projet\\Image-Inpainting-with-Basic-Convolutional-Networks\\checkpoints\\imagenet_128 exists and is not empty.\n",
      "Restoring states from the checkpoint path at checkpoints/imagenet_128/2024-12-17_08-44-43-epoch=38-val_loss=0.42.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                     | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | psnr_metric   | PeakSignalNoiseRatio     | 0      | train\n",
      "1 | joint_loss    | JointLoss                | 0      | train\n",
      "2 | rec_loss      | ReconstructionLoss       | 0      | train\n",
      "3 | adv_loss      | AdversarialLoss          | 0      | train\n",
      "4 | generator     | AdversarialGenerator     | 71.1 M | train\n",
      "5 | discriminator | AdversarialDiscriminator | 2.8 M  | train\n",
      "-------------------------------------------------------------------\n",
      "73.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "73.9 M    Total params\n",
      "295.556   Total estimated model params size (MB)\n",
      "58        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at checkpoints/imagenet_128/2024-12-17_08-44-43-epoch=38-val_loss=0.42.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43:   9%|â–‰         | 211/2306 [01:55<19:06,  1.83it/s, v_num=2, train_psnr=18.00, train_loss_context_encoder=0.216, train_loss_discriminator=2.46e-14, val_psnr=14.70, val_loss=0.419] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(model, ckpt_path\u001b[39m=\u001b[39mckpt_path)\n\u001b[0;32m    576\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_stage()\n\u001b[0;32m    983\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1025\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n\u001b[0;32m   1026\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance()\n\u001b[0;32m    206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_loop\u001b[39m.\u001b[39mrun(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:252\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 252\u001b[0m             batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n\u001b[0;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\manual.py:94\u001b[0m, in \u001b[0;36m_ManualOptimization.run\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mwith\u001b[39;00m suppress(\u001b[39mStopIteration\u001b[39;00m):  \u001b[39m# no loop to break at this level\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(kwargs)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\manual.py:114\u001b[0m, in \u001b[0;36m_ManualOptimization.advance\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39mkwargs\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m    115\u001b[0m \u001b[39mdel\u001b[39;00m kwargs  \u001b[39m# release the batch from memory\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 319\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    321\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 390\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mtraining_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\CYTechNVME\\AIIP_Projet\\Image-Inpainting-with-Basic-Convolutional-Networks\\src\\image_inpainting\\model\\context_encoder.py:136\u001b[0m, in \u001b[0;36mContextEncoder.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muntoggle_optimizer(generator_optimizer)\n\u001b[1;32m--> 136\u001b[0m psnr_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpsnr_metric(generator_outputs, true_masked_regions)\n\u001b[0;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mtrain_psnr\u001b[39m\u001b[39m'\u001b[39m, psnr_val, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\torchmetrics\\metric.py:312\u001b[0m, in \u001b[0;36mMetric.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_reduce_state_update(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\torchmetrics\\metric.py:381\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[1;32m--> 381\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    382\u001b[0m batch_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\torchmetrics\\metric.py:483\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 483\u001b[0m     update(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    484\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\torchmetrics\\image\\psnr.py:132\u001b[0m, in \u001b[0;36mPeakSignalNoiseRatio.update\u001b[1;34m(self, preds, target)\u001b[0m\n\u001b[0;32m    130\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclamping_fn(target)\n\u001b[1;32m--> 132\u001b[0m sum_squared_error, num_obs \u001b[39m=\u001b[39m _psnr_update(preds, target, dim\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim)\n\u001b[0;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\torchmetrics\\functional\\image\\psnr.py:79\u001b[0m, in \u001b[0;36m_psnr_update\u001b[1;34m(preds, target, dim)\u001b[0m\n\u001b[0;32m     78\u001b[0m sum_squared_error \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mpow(preds \u001b[39m-\u001b[39m target, \u001b[39m2\u001b[39m))\n\u001b[1;32m---> 79\u001b[0m num_obs \u001b[39m=\u001b[39m tensor(target\u001b[39m.\u001b[39mnumel(), device\u001b[39m=\u001b[39mtarget\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     80\u001b[0m \u001b[39mreturn\u001b[39;00m sum_squared_error, num_obs\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\CYTechNVME\\AIIP_Projet\\Image-Inpainting-with-Basic-Convolutional-Networks\\train_image_net_128.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CYTechNVME/AIIP_Projet/Image-Inpainting-with-Basic-Convolutional-Networks/train_image_net_128.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# trainer.fit(model, dm)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CYTechNVME/AIIP_Projet/Image-Inpainting-with-Basic-Convolutional-Networks/train_image_net_128.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CYTechNVME/AIIP_Projet/Image-Inpainting-with-Basic-Convolutional-Networks/train_image_net_128.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# To continue training:\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/CYTechNVME/AIIP_Projet/Image-Inpainting-with-Basic-Convolutional-Networks/train_image_net_128.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(model, dm, ckpt_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcheckpoints/imagenet_128/2024-12-17_08-44-43-epoch=38-val_loss=0.42.ckpt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m call\u001b[39m.\u001b[39m_call_and_handle_interrupt(\n\u001b[0;32m    539\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    540\u001b[0m )\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\image_inpainting\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[0;32m     63\u001b[0m         launcher\u001b[39m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[1;32m---> 64\u001b[0m     exit(\u001b[39m1\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n\u001b[0;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# trainer.fit(model, dm)\n",
    "\n",
    "# To continue training:\n",
    "trainer.fit(model, dm, ckpt_path=\"checkpoints/imagenet_128/2024-12-17_08-44-43-epoch=38-val_loss=0.42.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook \"tensorboard\" exists if you want to check how the metrics evolve during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on the last epoch\n",
    "\n",
    "Here the results of this cell are after ????????? epochs on the full Image Net dataset cropped and resized to 128x128.\n",
    "\n",
    "- **Number of steps**: \n",
    "- **Time**: \n",
    "- **Observations (with tensorboard)**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup(\"test\") # in case \"test\" wasn't called before this cell\n",
    "\n",
    "x, y = next(iter(dm.test_dataloader()))\n",
    "x = x.to(model.device)\n",
    "y = y.to(model.device)\n",
    "\n",
    "out = model.forward(x)\n",
    "\n",
    "print_results_images(x, y, out, \"Results on test set\", dm.inverse_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup(\"fit\") # in case \"fit\" wasn't called before this cell\n",
    "\n",
    "x, y = next(iter(dm.train_dataloader()))\n",
    "x = x.to(model.device)\n",
    "y = y.to(model.device)\n",
    "out = model.forward(x)\n",
    "\n",
    "print_results_images(x, y, out, \"Results on training set\", dm.inverse_transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_inpainting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
