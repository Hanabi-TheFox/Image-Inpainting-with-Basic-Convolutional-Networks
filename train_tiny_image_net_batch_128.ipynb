{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import pytorch_lightning as pl\n",
    "from image_inpainting.datamodule.tiny_image_net_data_module import TinyImageNetDataModule\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from image_inpainting.model.context_encoder import ContextEncoder\n",
    "from image_inpainting.utils import print_results_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "dm = TinyImageNetDataModule(\n",
    "    data_dir=os.path.join(data_dir, \"tiny-imagenet-200\"), \n",
    "    batch_size_train=128,\n",
    "    batch_size_val=128,\n",
    "    batch_size_test=128,\n",
    "    num_workers=10, \n",
    "    pin_memory=True, \n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a ContextEncoder model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContextEncoder(input_size=(3, 128, 128), hidden_size=4000, save_image_per_epoch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or load it from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ContextEncoder.load_from_checkpoint(\"checkpoints/tiny_imagenet/2024-12-16_14-34-46-epoch=76-val_loss=0.32.ckpt\") # change the path to your checkpoint\n",
    "# model.enable_save_image_per_epoch()\n",
    "# model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints/tiny_imagenet',\n",
    "    filename=now+'-{epoch:02d}-{val_loss:.2f}',\n",
    "    monitor='val_loss',\n",
    "    save_top_k=-1,  # Save all checkpoints\n",
    "    every_n_epochs=1  # Save checkpoint every n epochs\n",
    ")\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(\"Context_Encoder_Inpainting\")\n",
    "trainer = pl.Trainer(max_epochs=300, devices=-1, accelerator=\"cuda\", logger=tb_logger, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook \"tensorboard\" exists if you want to check how the metrics evolve during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display some images and evaluate the model performances\n",
    "\n",
    "Here the results of this cell are after 100 epochs on the Tiny Image Net dataset (10x less images compared to Image Net) in 64x64\n",
    "\n",
    "- **Number of steps**: 19 599\n",
    "- **Time**: 3h45\n",
    "- **Observation (with tensorboard)**: Note that this dataset is quite small compared to the full Image Net, we have 100 000 images. We can clearly see the overfitting after step 2 000 (after around 23 min, around epoch 10). So in this cell we will plot the last epoch and then in the following one we will plot the epoch with the best validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, dm)\n",
    "\n",
    "x, y = next(iter(dm.test_dataloader()))\n",
    "    \n",
    "x = x.to(model.device)\n",
    "y = y.to(model.device)\n",
    "\n",
    "out = model.forward(x)\n",
    "\n",
    "print_results_images(x, y, out, \"Results on test set\", dm.inverse_transform)\n",
    "\n",
    "dm.setup(\"fit\") # in case \"fit\" wasn't called before this cell\n",
    "\n",
    "x, y = next(iter(dm.train_dataloader()))\n",
    "x = x.to(model.device)\n",
    "y = y.to(model.device)\n",
    "out = model.forward(x)\n",
    "\n",
    "print_results_images(x, y, out, \"Results on training set\", dm.inverse_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on the best validation loss model\n",
    "\n",
    "Here the results of this cell are after 10 epochs on the Tiny Image Net dataset (10x less images compared to Image Net) in 64x64.\n",
    "\n",
    "- **Number of steps**: around 2 000\n",
    "- **Time**: around 20min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, dm)\n",
    "\n",
    "x, y = next(iter(dm.test_dataloader()))\n",
    "    \n",
    "x = x.to(model.device)\n",
    "y = y.to(model.device)\n",
    "\n",
    "out = model.forward(x)\n",
    "\n",
    "print_results_images(x, y, out, \"Results on test set\", dm.inverse_transform)\n",
    "\n",
    "dm.setup(\"fit\") # in case \"fit\" wasn't called before this cell\n",
    "\n",
    "x, y = next(iter(dm.train_dataloader()))\n",
    "x = x.to(model.device)\n",
    "y = y.to(model.device)\n",
    "out = model.forward(x)\n",
    "\n",
    "print_results_images(x, y, out, \"Results on training set\", dm.inverse_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "The first epochs are still blurry, so even if the loss is lower, the results is actually less realistic. On the other hand the last epoch is less blurry there is still some noise. It's especially visible in the logged image (validation set) that are not displayed here but that you can find using the tensorboard notebook and the GIF create_output_per_epoch_animated_result notebook.\n",
    "\n",
    "The other tests (other notebooks) will try with a bigger dataset (Image Net) and with different parameters on Tiny Image Net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_inpainting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
